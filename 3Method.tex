\section{Method}
\textcolor{blue}{Add an introduction to how the methods section is split into to parts adn what each will cover}.

\subsection{Producing a Residual Map and Performing Source Detection}
\textcolor{blue}{this section desperately needs some sub sub sections that can help structure the section}
A residual map is produced by modelling the light profiles of the detected sources and subtracting them from the scientific telescope image. Both the COSMOS 2015 and 2020 contain the information necessary to compute a residual map, but differ in their method of performing source extraction and thus also in their method of modelling sources. Since we are interested in objects that are visible in the IRAC bands and not visible in the NIR bands, we aim to produce a residual map of the IRAC image subtracting sources detected in the bluer bands. \\

In the COSMOS 2015 catalogue, the object photometry is carried out using the Source Extraction software  \cite[SExtractor,][]{SExtractor_1996}. To understand how the software works, I performed the source detection using SEP \cite{SEP_2018} (a Python wrapper for SExtractor) on the telescope images from three bands: two optical bands ($g$ and $i$) from HSC\footnote{The Hypersurprime Camera mounted on Subaru} and the near-IR band $Ks$ from VISTA\footnote{The Visible and Infrared Survey Telescope for Astronomy}. The code is available in the notebook \textcolor{darkgray}{Source\_Detection.ipynb} in the linked repository. \textcolor{red}{Should I even reference to the code like this or is it unnecessary?}. To perform proper source detection, multiple steps are involved: background estimation, detection thresholds and aperture photometry. First, we need to produce background estimation, since each pixel in a telescope image is the sum of background noise and flux from the object we are interested in. Background estimation consists of creating a background noise map, mapping the background flux level in different areas of the image. Subtracting this background map, we can thus produce a "clean" image, where the background is $\sim 0$ in native flux units so the pixel flux values within a source only includes the flux we are interested in. There are two main parameters that control the detection of sources: the deblending threshold and the flux threshold. 
\begin{wraptable}{r}{0.35\textwidth}
    \begin{tabular}{lc}
    \hline
    \textbf{Camera} & \textbf{Zero Point [mag]} \\ \hline
    HSC &  31.4 \\ \hline
    VISTA &  30 \\ \hline
    IRAC &  21.58 \\ \hline
    \end{tabular}
    \caption{Zero point magnitudes to convert flux  densities (in the image's native units) into magnitudes for the listed bands. \textcolor{blue}{perhaps a reference to the numbers, REMEMBER to actually write the numbers}}
    \label{zero_point}
\end{wraptable}
The deblending threshold controls when to split objects that are very close in the image and the flux threshold determines the signal to noise ratio (S/N) required for a detection. For example, a very low flux threshold will lead to the detection of spurious objects while a high threshold will overlook fainter objects. These steps will provide the coordinates of the detected objects along with a peak flux corresponding  to the value of the pixel with the highest flux. To find the integrated flux and the magnitude of the objects aperture photometry is used. The COSMOS 2015 catalogue uses fixed apertures diameters of respectively 2'' and 3'' and computes the total flux as the sum of all pixels within the aperture in the cleaned image (where the background map is subtracted). Another option is to use variant apertures depending on the size of the sources, which is what I used in my code. I computed the aperture diameter using the limits \textcolor{darkgray}{xmax, xmin, ymax} and \textcolor{darkgray}{ymin} from the SEP output to find the mean extent of the source along the two axes. A visual representation of the detected sources marked with their variant aperture can be found in fig. \ref{Source_detection} and \ref{Source_detection_Zoom} in the appendix. The AB magnitude is found with the formula\footnote{Refer to p. 29 in the MBW book}:
\begin{equation}
    m_X = -2.5\log_{10}(F_X) + m_{0,X}
\end{equation}
where $m_X$ is the computed magnitude, $F_X$ is the measured flux in the band in native units and $m_{0,X}$ is the zero-point particular for the image observed, which converts the native flux units into physical units. In this project, we have used the zero-point magnitudes listed in table \ref{zero_point}. Computing the magnitudes with this method, we are able to compare our results with the $2^{\prime\prime}$ aperture magnitudes reported in the COSMOS 2015 catalogue. The distributions of magnitudes in respectively the catalogue and in our result are illustrated in fig.\ref{mag_hist}, along with the difference in magnitude between corresponding objects plotted in a residual histogram. The residuals are centred at 0 and have a low spread, indicating that our measurement of flux agrees with the measurements reported in the catalogue.
\begin{wrapfigure}{R}{0.5\textwidth}
    \centering %left, lower, right, upper
    \includegraphics[trim={0.5cm 0 2cm 1.8cm},clip,width=0.45\textwidth]{Code/Saved_Figures/Mag_hist.pdf}
    \caption{The distribution of magnitudes pertaining to objects in the COSMOS field. In blue, the magnitude measurements found by running SEP is plotted. In orange the 2'' magnitudes reported in the COSMOS 2015 catalogue are displayed. In grey a residual plot is shown, mapping the difference between the measured and reported magnitude for each object.}
    \label{mag_hist}  
\end{wrapfigure}

Utilising the software IRACCLEAN presented in \cite{Hsieh_2012_IRACCLEAN}, the sources can be modelled to produce a residual map. The centroids from the SEP algorithm and the total flux found from fixed aperture photometry is used to create point source models of the sources. The point source model is an approximation, since the light profiles of the galaxies might differ from a perfect point source. However, due to the low resolution of the IRAC image, as a result of its peculiarly large point spread function (PSF) that is much bigger than the size of the galaxies modelled, the approximation is deemed reasonable. \textcolor{red}{I should probably make it clear that I didnt actually do this but refer to the author - Iary. Also convolving with PSF?}. \\ \\
There are, however, some limitations to aperture photometry and the residual map it produces. All sources in the image are modelled individually with a point source model, where the flux is a free parameter that rescales the profile to fit the source. The flux used is the total flux within a fixed aperture, hence the aperture size is an assumption we enforce on all sources, even though it is not a one size fits all problem. Typically this is not a problem as long as the aperture size is generous, including all flux intrinsic to the source, and the surrounding is background which values are centred around zero. The problem arises when there is a severely de-blended pair of galaxies, and the intrinsic flux cannot be properly estimated or divided between the sources \textcolor{red}{consider moving this to the discussion}. To address the drawbacks of aperture photometry, we explore a different approach of creating a residual image. An alternative to aperture photometry is model-based photometry, a method from which the Farmer tool is developed. \\

The Farmer tool \cite{Weaver_2020}, like the previous method, also starts out with a source detection algorithm (SEP) reporting the centroids of each source detected. Next it identifies particularly crowded regions where there are multiple objects nearby each other that could present overlapping flux. Contrary to the previous methods, these sources are modelled simultaneously so as to not count the same flux twice for nearby pairs of sources. Furthermore this method does not require enforcing a fixed aperture size. Instead the total flux is determined by fitting an intrinsic light profile to the source, and integrating the area below the surface. The Farmer tool fits a source to one of 5 models. Common for all of them is that they are parametrized using the centroid position of the source as a fixed parameter and the total flux as a free parameter. The models are:
\begin{enumerate}
    \item \textbf{Point Source} models are taken directly from the point spread function (PSF) used. It is similar to the method applied when using aperture photometry.
    \item \textbf{Simple Galaxy} models are circular symmetric, exponential light profile with a fixed 0.45’’ effective radius.
    \item \textbf{Exponential Galaxy} models are exponential light profiles. In addition to the common parameters these are parametrised also by effective radius, axis ratio and position angle.
    \item \textbf{DeVaucouleur Galaxy} models are de Vaucouleurs light profiles. These models require similar parameters as the exponential galaxy model.
    \item \textbf{Composite Galaxy} models are combines a Exponential Galaxy profile with a DeVaucouleur Galaxy profile. The profiles are concentric, and the model, like the others, is therefor only parametrised by one pair of centroids coordinates. Each component is parametrised by the free parameters described in the two previous models. In addition there is a “fraction of total flux” parameter that distributes the flux between the two components.
\end{enumerate}


Except for the Point Source model, they can all be described analytically by changing the parameter $n$ in a 2d Sérsic profile, which is given by \footnote{\textcolor{blue}{better reference perhaps}\url{http://ned.ipac.caltech.edu/level5/March05/Graham/Graham2.html}} eq. \ref{sersic},
\begin{equation}
   I(R) = I_e \cdot \exp \left(-b_n \cdot \left( \left( \frac{R}{R_e} \right)^{\frac{1}{n}} - 1\right) \right)
   \label{sersic}
\end{equation}
where $I_e$ is the intensity at the effective radius $R_e$ that encloses half of the total light from the model. The constant $b_n$ is defined from $n$, to make sure the total luminosity is obtained when integrating the profile. Since they are all analytical, they can easily be integrated to obtain the total flux. The real challenge lies in choosing the right model and obtaining a converging fit. The Farmer tool includes a decision tree for selecting the most appropriate model type to fit a given source with. The models are tested in the order they are described above (1 to 5), and the quality of their residuals are ranked, so the best model is chosen. When the most appropriate model type is selected for all sources, optimisation of the free parameters is performed as the final step, so that the parameters chosen for one object will not directly affect the parameters of nearby objects. \textcolor{red}{Next sentence might also be more suited for the discussion} While this method does improve on some of the drawbacks from aperture photometry, it is next to impossible to make all model fittings converge. There are a few possible reasons for the failure to converge. Sometimes a bright source is simply not well described by a smooth profile. It could also be due to a very blended pair that was not successfully separated in the detection, and thus not described correctly by a single light profile. \\
With this process, the Farmer tool can provide us with the parameters needed for creating intrinsic light profiles of the sources detected in the image. The models describe how we expect the light to look intrinsically, but when the light is observed through a telescope it is warped due to the way the imaging system responds to a point source. A point spread function (PSF) maps how a point-like object will appear in the image you are working with. Therefore, to create a residual image we need to model the objects as they would appear in the image. We obtain the final models from a convolution of the intrinsic light profiles with the PSF of the image. The residual map is then produced by subtracting the convoluted models from the telescope image, carefully placing the models right according to the detected centroid coordinates.

For the next part of the project we need a catalogue containing the coordinates and total flux of objects detected in the residual image. For this purpose we use SEP source detection and aperture photometry to estimate the total flux. This catalogue will likely contain different objects of varying interest: spurious objects, left-over flux from subtracted sources due to imperfections in the residual image, and hopefully there will be some dropout galaxies.

\subsection{Classification with Semi-supervised Machine Learning}
In this thesis we develop a semi-automated method of selecting NIR-dropout galaxies based on t-distributed Stochastic Neighbour Embedding (t-SNE) \cite{Maaten_2008_tSNE} and k Nearest Neighbour (kNN) voting. This method uses as input: 4 telescope images \textcolor{red}{telescope cutouts/ images with the galaxy centred/poststamps/vignettes - find the right description} of each galaxy in the sample, from respectively the $H$, $Ks$, $ch1$ and $ch2$ bands. The method is semi-supervised, and hence will need some parameter tuning along the way and some labels. With this, the technique will predict whether each galaxy in the sample is a dropout or not.

\subsubsection{Dimensionality Reduction with t-SNE}
As mentioned, the input that the method is given for each object in the sample is four small telescope images centred at the object in question. We use cutouts of the size 21x21 pixels, meaning that we in total will have $21\cdot21\cdot4=1764$ parameters for each object in the sample. Inherently, this is a very high-dimensional classification problem, and it is particularly difficult to produce meaningful predictions from, when the learning is unsupervised. A way to simplify the problem is embedding the high dimensional data in a 2 dimensional representation. There are many approaches to obtaining such an embedding, but for our purposes we will adopt the method of t-SNE, which has gained a lot of popularity since its release in 2008 \textcolor{blue}{quantify this more how many paper used it in astrophysics etc}. A recent NBI paper \cite{Steinhardt_2020}, proposed this novel technique for classification of galaxies from photometric data, which has inspired the approach applied here. In particular they used t-SNE to classify quiescent galaxies from their spectral energy distributions (SEDs). \\

t-SNE takes a set of high dimensional vectors, in this case the pixels originating from four different images, and computes the euclidean distance between the each vectors. From this it calculates a probability that two vectors should be considered neighbours. This probability considers the user-specified hyper parameter \textit{perplexity}, which determines the sizes of the neighbourhoods based on the density of the data in the respective regions. The hyper parameter can effectively be understood as a estimate for the number of neighbours that should be considered similar. Values between 1 and 50 is suggested by the author \cite{Maaten_2008_tSNE}. A low perplexity promotes a local structure in the embedding more, while a high perplexity will preserve more of the global structure of the samples. A universally good perplexity value cannot be determined, but should be selected manually for each data set. This is because the density of the objects varies depending on the sample size, and thus the number of optimal neighbours will vary. The perplexity is chosen, in this thesis, by performing the t-SNE embedding for various perplexities and determining which structure is best suited for the purpose of classifying dropouts. \\
t-SNE is stochastic in its nature, and will thus compute different embeddings for the same data set unless a random seed is specified. To be able to reproduce our results, this was therefore implemented. Its stochastic nature lies in the fact that it will try out different configurations for the embedding and evaluate the probabilities, which it will try to improve in the next iteration. Due to the huge dimension of the data, this makes the process much faster, but also means that the embedding is not reversible, and new objects cannot be added individually to the embedding space. All data thus has to be given to the algortihm at once. We can specify the maximum number of iterations for the procedure, and it will output the number of iterations it actually used to produce the embedding. This means we can also use the number of iterations as a tool for choosing the right perplexity. If the number of iterations is smaller than the maximum number of iterations, this means the embedding has converged in the sense that it could not find a better configuration for the embedding during the next 300 iterations. \\
Since the t-SNE algortithm is a form of unsupervised machine learning, the embedding map will be produced without any direct knowledge of what a dropout galaxy looks like. It will however place objects it considers similar close to each other, even though the algorithm is unaware that the parameters are pixels. This is also one of the reasons t-SNE is so popular. The flexibility in its applications makes it a very powerful tool\textcolor{red}{perhaps mentions of where it is also used other than astronomy}. For our purpose it is useful because the photometric information that is stored in the difference in the image cutouts is an indicator of dropout galaxies.

\subsubsection{The Use of Tracers in the Embedding}
So far the classification process has been completely unsupervised, but now that a 2 dimensional representation of the sample is obtained, we need an indication of which region too look for dropout galaxies in. By selecting "tracers" in our sample, we can map where they are placed in the embedding by the t-SNE algorithm. If the tracers cluster well, this is a good indication of a promising region. \\

Tracers is what we call the objects we have determined to be reliable candidates of NIR-dropouts. These are selected based on a visual inspection of the objects in the four bands. We look for objects that are visible in IRAC $ch1$ and $ch2$, and which do not appear in the NIR bands $H$ and $Ks$. Since the objects we look for are similar to those found in \cite{Wang_2019} and \cite{Alcalde_Pampliega_2019}, we can also use these as a reference. An example of which objects we would classify as a tracer is displayed in fig. \ref{noget}. \textcolor{blue}{proper figure reference}.


This is where the semi-supervised part comes to play.



\begin{itemize}
    \item Describe our use of tracers that were found by visual inspection. Looking to the detected objects in the different bands we could see if they looked like H-dropout candidates. We assigned good labels to these. \textcolor{red}{Either here or perhaps in the introduction we should describe how we expect the candidates to look like, what features are we looking for when we visually inspect them}
    \item Describe the classification part: K nearest neighbour voting based on the euclidean distance. Elaborate on this choice: we also tried using all neighbours within a radius but due to the t-SNE embedding that essentially projects a higher dimensional manifold into 2d the distances are not necessarily euclidean, different regions has different densities. Varying the fraction of votes "for" to produce ROC curve.
    \item Score metric: ROC curve and AUC, accuracy, and purity/contamination rate.
\end{itemize}